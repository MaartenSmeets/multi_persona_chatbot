# src/config/llm_config.yaml
api_url: "http://localhost:11434/api/generate"  # Replace with your Ollama API endpoint
model_name: "Euryale-v2.3:latest"  # Specify the model version
api_key: ""  # Optional: Include if authentication is required
max_retries: 3  # Number of retries for LLM requests
temperature: 0.7  # Default temperature
max_context_length: 128256  # Max context length before summarizing
